{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2016 Shangtong Zhang(zhangshangtong.cpp@gmail.com)                  #\n",
    "# 2016 Kenta Shimada(hyperkentakun@gmail.com)                         #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################\n",
    "#######################################################################\n",
    "# Copyright (C)                                                       #\n",
    "# 2017 Cheung Auyeung(cheung.auyeung@gmail.com)                       #\n",
    "# Permission given to modify the code as long as you keep this        #\n",
    "# declaration at the top                                              #\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A python decorator to time the elapse time of a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "\n",
    "        print('%r  %2.2f s' % \\\n",
    "            (method.__name__, (te - ts) ))\n",
    "        return result\n",
    "\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World Example\n",
    "```\n",
    " +---+---+---+---+---+\n",
    " |   | A |   | B |   |\n",
    " +---+---+---+---+---+\n",
    " |   |   |   |   |   |\n",
    " +---+---+---+---+---+\n",
    " |   |   |   | B'|   |\n",
    " +---+---+---+---+---+\n",
    " |   |   |   |   |   |\n",
    " +---+---+---+---+---+\n",
    " |   | A'|   |   |   |\n",
    " +---+---+---+---+---+ \n",
    "```\n",
    "\n",
    "Above is a 5x5 rectangular gridworld representation of a simple finite MDP. \n",
    "The cells of the grid correspond to the states of the **environment**. It has designated location A, A', B and B' for specific behavior of the enviroment to be explained later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WORLD_SIZE = 5\n",
    "\n",
    "A_POS = [0, 1]\n",
    "A_PRIME_POS = [4, 1]\n",
    "B_POS = [0, 3]\n",
    "B_PRIME_POS = [2, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible actions (decision) of the agent at it's current cell (state):\n",
    "* Left, Up, Right, Down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# left, up, right, down\n",
    "actions = ['L', 'U', 'R', 'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action is the decision make by the agent to move in the respective direction to the next cell. Because of the nature of the environment, the agent may or or may not be able to move in the decided direction. \n",
    "\n",
    "The enviroment specifies the next cell and the next reward as a result of the action taken by the agent at a cell according to the followings: \n",
    "* If the action would take the agent off the grid, the agent shall stay in the same cell and the agent shall receive a reward of -1.\n",
    "* Else if the agent is at A, the agent shall jump to A' and recive a reward of 10, for all action.\n",
    "* Else if the agent is at B, the agent shall jump to B' and recive a reward of 5, for all action.\n",
    "* Else, the agent moves by one cell in the direction of the action and receive a reward of 0.\n",
    "\n",
    "**Note** In this example, the next state $s'(s,a)$ (i.e. `nextSate`) and reward $r(s,a)$ (i.e, `actionReward`) are deterministic function of the current state and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nextState = []\n",
    "actionReward = []\n",
    "for i in range(0, WORLD_SIZE):\n",
    "    nextState.append([])\n",
    "    actionReward.append([])\n",
    "    for j in range(0, WORLD_SIZE):\n",
    "        next = dict()\n",
    "        reward = dict()\n",
    "        if i == 0:\n",
    "            next['U'] = [i, j]\n",
    "            reward['U'] = -1.0\n",
    "        else:\n",
    "            next['U'] = [i - 1, j]\n",
    "            reward['U'] = 0.0\n",
    "\n",
    "        if i == WORLD_SIZE - 1:\n",
    "            next['D'] = [i, j]\n",
    "            reward['D'] = -1.0\n",
    "        else:\n",
    "            next['D'] = [i + 1, j]\n",
    "            reward['D'] = 0.0\n",
    "\n",
    "        if j == 0:\n",
    "            next['L'] = [i, j]\n",
    "            reward['L'] = -1.0\n",
    "        else:\n",
    "            next['L'] = [i, j - 1]\n",
    "            reward['L'] = 0.0\n",
    "\n",
    "        if j == WORLD_SIZE - 1:\n",
    "            next['R'] = [i, j]\n",
    "            reward['R'] = -1.0\n",
    "        else:\n",
    "            next['R'] = [i, j + 1]\n",
    "            reward['R'] = 0.0\n",
    "\n",
    "        if [i, j] == A_POS:\n",
    "            next['L'] = next['R'] = next['D'] = next['U'] = A_PRIME_POS\n",
    "            reward['L'] = reward['R'] = reward['D'] = reward['U'] = 10.0\n",
    "\n",
    "        if [i, j] == B_POS:\n",
    "            next['L'] = next['R'] = next['D'] = next['U'] = B_PRIME_POS\n",
    "            reward['L'] = reward['R'] = reward['D'] = reward['U'] = 5.0\n",
    "\n",
    "        nextState[i].append(next)\n",
    "        actionReward[i].append(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 3.5  Grid Word Example with Equiprobable Random Policy\n",
    "\n",
    "The policy of an agent is the  probability $\\pi(a\\,|\\,s)$ that the agent shall take action $a$ at state $s$.  For Figure 3.5, $\\pi(a\\,|\\,s)$ is `actionProb`, which is equiprobable in all directions (action) and at all cell locations (state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actionProb = []\n",
    "for i in range(0, WORLD_SIZE):\n",
    "    actionProb.append([])\n",
    "    for j in range(0, WORLD_SIZE):\n",
    "        actionProb[i].append(dict({'L':0.25, 'U':0.25, 'R':0.25, 'D':0.25}))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this equiprobable random policy $\\pi(s\\,|\\,a)$ and discount rate $\\gamma = 0.9$ for the reward, the value function $v_\\pi(s)$, `world`, is computed by solving the system of linear equations (3.12) iteratively.\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a} \\pi(a\\,|\\,s) \\sum_{s',r} p(s',r\\,|\\,s,a) \n",
    "\\left[ r + \\gamma v_\\pi(s') \\right]\n",
    "$$\n",
    "\n",
    "Since the mapping from $(s,a)$ to the next state and reward $(s',r)$ is deterministic, we have\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a} \\pi(a\\,|\\,s) \\left[ r(s,a) + \\gamma v_\\pi(s'(s,a)) \\right]\n",
    "$$\n",
    "where the reward $r(s,a)$ and the next state $s'(s,a)$ are deterministic functions of $(s,a)$.\n",
    "\n",
    "The solution $v_\\pi(s)$ for all $s$ is a fixed point of the equation. It is computed iteratively by the following algorithm:\n",
    "1. At $n=0$, initialize $v_\\pi(s_n) = 0$ (the vector `world`) by the following code: \n",
    "```python\n",
    "world = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "```\n",
    "2. Given $s_{n}$, update $v_\\pi(s_{n+1})$ (the vector `newWorld`),   \n",
    "\n",
    "  $$\n",
    "  v_\\pi(s_{n+1}) = \\sum_{a} \\pi(a\\,|\\,s_n) \\left[ r(s_n,a) + \\gamma v_\\pi(s'(s_n,a)) \\right]\n",
    "  $$\n",
    "  by the following code:\n",
    "```python \n",
    "newWorld[i, j] += actionProb[i][j][action] * \n",
    "    (actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]])\n",
    "```\n",
    "3. Repeat (2) until it is converged, i.e., the L1 norm $||v_\\pi(s_{n+1}) - v_\\pi(s_{n}||_1$, is sufficient small, by the following code:\n",
    "```python\n",
    "np.sum(np.abs(world - newWorld)) < 1e-4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def figure3_5() :\n",
    "    discount = 0.9\n",
    "    world = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    n = 0\n",
    "    while True:\n",
    "        # keep iteration until convergence\n",
    "        newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "        for i in range(0, WORLD_SIZE):\n",
    "            for j in range(0, WORLD_SIZE):\n",
    "                for action in actions:\n",
    "                    newPosition = nextState[i][j][action]\n",
    "                    # bellman equation\n",
    "                    newWorld[i, j] += actionProb[i][j][action] * (\n",
    "                        actionReward[i][j][action] + \n",
    "                        discount * world[newPosition[0], newPosition[1]] )\n",
    "        n += 1\n",
    "        if np.sum(np.abs(world - newWorld)) < 1e-4:\n",
    "            print(\n",
    "                'Value Function under the Random Policy found after {} iterations:'.format(n))\n",
    "            return newWorld\n",
    "        world = newWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function under the Random Policy found after 77 iterations:\n",
      "[[ 3.3  8.8  4.4  5.3  1.5]\n",
      " [ 1.5  3.   2.3  1.9  0.5]\n",
      " [ 0.1  0.7  0.7  0.4 -0.4]\n",
      " [-1.  -0.4 -0.4 -0.6 -1.2]\n",
      " [-1.9 -1.3 -1.2 -1.4 -2. ]]\n"
     ]
    }
   ],
   "source": [
    "state_value = figure3_5()\n",
    "print(np.array_str(state_value, precision=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3.8 Grid Word Example with Optimal Policy\n",
    "\n",
    "Figure 3.8 also try to solve the same grid world problem in Figure 3.5 with discount rate $\\gamma = 0.9$ for the reward but with optimal policy.\n",
    "\n",
    "The value function with the optimal policy is found by the\n",
    "Bellman optimalality equation for $v_\\ast(s)$ \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\ast(s) \\,=\\, &\\underset{a}{\\mathrm{argmax}}\\, q_{\\pi_\\ast}(s,a) \\\\\n",
    " \\,=\\, &\\underset{a}{\\mathrm{argmax}}\\, \\sum_{s',r} p(s',r\\,|\\,s,a) \\left[ r + \\gamma v_\\ast(s') \\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the mapping from $(s,a)$ to the next state and reward $(s',r)$ is deterministic, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "v_\\ast(s) \\,=\\, &\\underset{a}{\\mathrm{argmax}}\\, q_{\\pi_\\ast}(s,a) \\\\\n",
    " \\,=\\, &\\underset{a}{\\mathrm{argmax}}\\, \\left[ r(s,a) + \\gamma v_\\ast(s'(s,a)) \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "where the reward $r(s,a)$ and the next state $s'(s,a)$ are deterministic functions of $(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution $v_\\ast(s)$ for all $s$ is a fixed point of the equation. It is computed iteratively by the following algorithm:\n",
    "1. At $n=0$, initialize $v_\\ast(s_n) = 0$ (the vector `world`) by the following code: \n",
    "    ```python\n",
    "    world = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    ```\n",
    "2. for every $s_{n}$ (i.e. every grid position `[i][j]` in the $n$-th iteration, compute the `values`  :    \n",
    "\n",
    "  $$\n",
    "     q_{\\pi_\\ast}(s_n,a) = r(s,a) + \\gamma v_\\ast(s'(s_n,a))\n",
    "  $$ \n",
    "   \n",
    "   for an action $a$ (i.e. `[action]`) by the following code :\n",
    "```pyhon \n",
    "values.append(\n",
    "    actionReward[i][j][action] + discount * world[newPosition[0], newPosition[1]])\n",
    "```\n",
    "Then update $v_\\ast(s_{n+1})$ for interation $n+1$, (the vector `newWorld`), \n",
    "    \n",
    "    $$\n",
    "    v_\\ast(s_{n+1}) = \\underset{a}{\\mathrm{argmax}}\\, q_{\\pi_\\ast}(s_n,a)\n",
    "    $$\n",
    "    by the following code:\n",
    "```python           \n",
    "newWorld[i][j] = np.max(values)\n",
    "```\n",
    "3. Repeat (2) until it is converged, i.e., the L1 norm $||v_\\ast(s_{n+1}) - v_\\ast(s_{n}||_1$, is sufficient small, by the following code:\n",
    "```python\n",
    "np.sum(np.abs(world - newWorld)) < 1e-4\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def figure3_8() :\n",
    "    discount = 0.9\n",
    "    world = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "    n = 0\n",
    "    while True:\n",
    "        # keep iteration until convergence\n",
    "        newWorld = np.zeros((WORLD_SIZE, WORLD_SIZE))\n",
    "        for i in range(0, WORLD_SIZE):\n",
    "            for j in range(0, WORLD_SIZE):\n",
    "                values = []\n",
    "                for action in actions:\n",
    "                    newPosition = nextState[i][j][action]\n",
    "                    # value iteration\n",
    "                    values.append(\n",
    "                        actionReward[i][j][action] + \n",
    "                        discount * world[newPosition[0], newPosition[1]])\n",
    "                newWorld[i][j] = np.max(values)\n",
    "        n += 1\n",
    "        if np.sum(np.abs(world - newWorld)) < 1e-4:\n",
    "            print('Optimal Value Function found after {} iterations:'.format(n))            \n",
    "            return(newWorld)\n",
    "        world = newWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function found after 124 iterations:\n",
      "[[ 22.   24.4  22.   19.4  17.5]\n",
      " [ 19.8  22.   19.8  17.8  16. ]\n",
      " [ 17.8  19.8  17.8  16.   14.4]\n",
      " [ 16.   17.8  16.   14.4  13. ]\n",
      " [ 14.4  16.   14.4  13.   11.7]]\n"
     ]
    }
   ],
   "source": [
    "optimal_state_value = figure3_8()\n",
    "print(np.array_str(optimal_state_value, precision=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
